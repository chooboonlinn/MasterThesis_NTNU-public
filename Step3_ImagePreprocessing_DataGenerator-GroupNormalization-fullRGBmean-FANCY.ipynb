{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pandas import DataFrame\n",
    "import random\n",
    "import functools\n",
    "import math\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from math import radians\n",
    "from math import pi\n",
    "import time\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./CommonFunctions_OthersGeneral.ipynb  ## This is important for library import, getting the path and runlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_mean():\n",
    "    runMean = []\n",
    "    runStd =[]\n",
    "    \n",
    "    for num in runlist:\n",
    "        all_images = []\n",
    "        filename_screenshot = (f\"Table_Run{num}.csv\")\n",
    "        run_table = pd.read_csv(dir6 + filename_screenshot, usecols = ['ScreenShotPath'])\n",
    "        \n",
    "        for row in run_table.itertuples():\n",
    "            all_images.append(mpimg.imread(row.ScreenShotPath))\n",
    "        all_images = np.array(all_images)\n",
    "    \n",
    "        # Full RGB mean\n",
    "        full_rgb_mean = np.mean(all_images, axis=(0,1))\n",
    "        full_rgb_std = np.std(all_images, axis=(0,1))    \n",
    "        \n",
    "        runMean.append(full_rgb_mean)\n",
    "        runStd.append(full_rgb_std)\n",
    "        \n",
    "    return runMean,runStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To generate the list before feeding into datagenerator ##\n",
    "\n",
    "def datagenerator_prep1_extractdata(runMean,runStd,runlist,grid_division,mode):\n",
    "    \n",
    "    if mode == 'training' or mode == 'learning':\n",
    "        chosen_angle = ['45°','135°','225°','315°'] # X\n",
    "        #chosen_angle = ['315°','0°/360°','45°','90°'] # Horizontal Upper\n",
    "        #chosen_angle = ['315°','0°/360°','45°'] # Horizontal Upper, just 3 angles\n",
    "        #chosen_angle = ['0°/360°','45°','90°','135°'] # Vertical right\n",
    "    elif mode == 'testing':\n",
    "        chosen_angle = ['0°/360°','90°','180°','270°'] # +\n",
    "        #chosen_angle = ['135°','180°','225°','270°'] # Horizontal lower\n",
    "        #chosen_angle = ['135°','180°','225°'] # Horizontal lower, just 3 angles\n",
    "        #chosen_angle = ['180°','225°','270°','315°'] # Vertical left\n",
    "    else: ## In case user forget to define the mode, the program will be halted\n",
    "        raise ValueError('Please select mode of paradigm')\n",
    "        pass\n",
    "    \n",
    "    run_data = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    for num in runlist: ## Do for every run ##\n",
    "        XY = [] ## The existing XY positions ##\n",
    "        XYnew = [] ## The newly divided XY positions ##\n",
    "        chosenCoordinates = [] ## To choose an array of old position that matches most closely with the new positions ##\n",
    "        \n",
    "        filename_screenshot = (f\"Table_Run{num}.csv\")\n",
    "        run_table = pd.read_csv(dir6 + filename_screenshot, usecols = ['Angle','X','Y','ScreenShotPath']) ## To read at the right csv table ##\n",
    "        maxX = max(run_table['X']) ## finding the maximum X in old positions ##\n",
    "        minX = min(run_table['X'])\n",
    "        maxY = max(run_table['Y']) ## finding the maximum Y in old positions ##\n",
    "        minY = min(run_table['Y'])\n",
    "\n",
    "        for rt in run_table.itertuples(): ## To make X,Y pairing tuple list ##\n",
    "            label_x = rt.X\n",
    "            label_y = rt.Y\n",
    "            XY.append((label_x, label_y)) ## To make each row of the X and Y into a tuple pair\n",
    "        XY = list(set(XY)) ## To remove duplication in list by converting it to set to speed up comparison later by shrinking the size 8 times ##\n",
    "\n",
    "        X = np.linspace(0,maxX,grid_division) ## To get the new X portion of division ##\n",
    "        Y = np.linspace(0,maxY,grid_division) ## To get the new X portion of division ##\n",
    "        mesh = np.meshgrid(X,Y) # To make even distribution across the map\n",
    "        X = mesh[0].flatten() # X-axis\n",
    "        Y = mesh[1].flatten() # Y-axis\n",
    "        XYnew = [[x, y] for x, y in zip(X, Y)] ## To convert each X and Y (in ascending order) into tuple pairs in one list: [(X1,Y1),(X2,Y2)...]\n",
    "            \n",
    "        for grid in XYnew: ## For each tuple XY pair in XYnew list\n",
    "            def distance(point_a, point_b): ## To define Xold - Xnew and Yold-Ynew (Teaching the program to do maths the way you define) ##\n",
    "                x0, y0 = point_a\n",
    "                x1, y1 = point_b\n",
    "                return math.fabs(x0 - x1) + math.fabs(y0 - y1)\n",
    "        \n",
    "            def nearest(grid, XY): ## To get the smallest value from Xold - Xnew and Yold-Ynew ##\n",
    "                distance_from_point = functools.partial(distance, grid)\n",
    "                return min(XY, key=distance_from_point)\n",
    "            chosenCoordinates.append(nearest(grid,XY)) ## To store the coordinates closest to each new position in an array of tuple pairings\n",
    "        \n",
    "        data = []\n",
    "    \n",
    "        for row in run_table.itertuples():\n",
    "            for x,y in chosenCoordinates:\n",
    "                if row.X == x and row.Y == y and any(row.Angle in angle for angle in chosen_angle):\n",
    "                    newArray = row.Index\n",
    "                    data.append(newArray) ## To form a list of indices of training data\n",
    "        data = run_table.iloc[data] # MF: Index into dataframe with chosen indices\n",
    "        \n",
    "        ## To rescale the X and Y\n",
    "        downScalingX_column = []\n",
    "        downScalingY_column = []\n",
    "        runMean_column = []\n",
    "        runStd_column = []\n",
    "        \n",
    "        for i in data.itertuples():\n",
    "            downScalingX = (i.X - minX)/(maxX - minX)\n",
    "            downScalingX_column.append(downScalingX)\n",
    "            downScalingY = (i.Y - minY)/(maxY - minY)\n",
    "            downScalingY_column.append(downScalingY)\n",
    "            runMeanValue = runMean[index]\n",
    "            runMean_column.append(runMeanValue)\n",
    "            runStdValue = runStd[index]\n",
    "            runStd_column.append(runStdValue)\n",
    "            \n",
    "        \n",
    "        data = data.assign(downScalingX = downScalingX_column)\n",
    "        data = data.assign(downScalingY = downScalingY_column)\n",
    "        data = data.assign(runMeanValue = runMean_column)\n",
    "        data = data.assign(runStdValue = runStd_column)\n",
    "        \n",
    "        run_data.append(data)\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "    return run_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To perform random check on the data (Original Image, Normalized Images, Histogram of these images)\n",
    "# Figure 2.6\n",
    "\n",
    "def normalized_illustration_randomcheck(run_data,samplingsize,mode):\n",
    "    data = random.choice(run_data)\n",
    "    row = data.sample(samplingsize)\n",
    "\n",
    "    oriImage_all = []\n",
    "    firstNorm_all = []\n",
    "    secondNorm_all = []\n",
    "    thirdNorm_all = []\n",
    "    \n",
    "    nt = time.strftime(\"%d-%m-%Y_%H%M\")\n",
    "        \n",
    "    folder = dir22 + '/' + test_runNum + '-' + mode + '_randomNormCheck_' + nt\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    for i in row.itertuples():\n",
    "        img = mpimg.imread(i.ScreenShotPath)\n",
    "        oriImage_all.append(img)\n",
    "\n",
    "        image = (img - i.runMeanValue) / (i.runStdValue) #+ 0.0001) # Group Normalisation # May get the error 'can only concatenate list (not \"float\") to list' if i add 0.0001 although the original intention of doing this is to avoid zero error during model training\n",
    "        image = image.astype(np.float32)\n",
    "        firstNorm_all.append(image)\n",
    "        \n",
    "        \n",
    "        # ImageNet Technique (Not necessary, because next step makes it into 0 to 1. 0 to 1 is preferred because negative values will become 0 weight in relu of Conv2D)\n",
    "        # But just placed in since previous testings take this algorithm into account, and no more time to rerun everything before report submission\n",
    "        # Advised to drop it if anyone from the lab is using my code unless your model activation does not change negative values into 0, then maybe you can try using this and cut out next standardization step\n",
    "        image /= 127.5 # To downscale it from (0-255) to \n",
    "        image -= 1.    # (-1 to 1)\n",
    "        secondNorm_all.append(image)\n",
    "\n",
    "        image = (image - np.min(image))/(np.max(image) - np.min(image)) # To convert the range into (0 to 1)\n",
    "        thirdNorm_all.append(image)\n",
    "\n",
    "    oriImage_all= np.asarray(oriImage_all)\n",
    "    thirdNorm_all= np.asarray(thirdNorm_all)\n",
    "    \n",
    "    imageLen = list(range(0,(oriImage_all.shape[0]),1))\n",
    "    \n",
    "    for i in imageLen:\n",
    "        fig, axes = plt.subplots(2,2, figsize=(15,8))#,constrained_layout=True)\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for idx, ax in enumerate(axes):\n",
    "            if idx == 0:\n",
    "                dataname = 'OriginalImage'\n",
    "                img = oriImage_all[i]\n",
    "                ax.set_title(dataname, pad=20, fontsize=14, fontweight='bold')                          \n",
    "                ax.matshow(img)\n",
    "            elif idx == 1:\n",
    "                dataname = 'OriginalImage_Histogram'\n",
    "                img = oriImage_all[i]\n",
    "                ax.set_title(dataname, pad=20, fontsize=14, fontweight='bold')\n",
    "                ax.xaxis.set_ticks_position('top')\n",
    "                ax.hist(img.flatten())\n",
    "            if idx == 2:\n",
    "                dataname = 'NormalizedImage' #'3rd_Normalize'\n",
    "                img = thirdNorm_all[i] #[idx]\n",
    "                ax.set_title(dataname, pad=20, fontsize=14, fontweight='bold')                \n",
    "                ax.matshow(img)\n",
    "            elif idx == 3:\n",
    "                dataname = 'NormalizedImage_Histogram' #'3rd_Normalize_Histogram'\n",
    "                img = thirdNorm_all[i]\n",
    "                ax.set_title(dataname, pad=20, fontsize=14, fontweight='bold')\n",
    "                ax.xaxis.set_ticks_position('top')\n",
    "                #ax.set_xticks([0.0,0.0,0.2,0.4,0.6,0.8,1.0,1.0])\n",
    "                #ax.set_xticklabels(['0.0','0.0','0.2','0.4','0.6','0.8','1.0','1.0'])\n",
    "                ax.hist(img.flatten())\n",
    "                \n",
    "        #GeneralTitle = 'Random Visualization for Normalization' #(f\"Run{num} n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")\n",
    "        #fig.suptitle((GeneralTitle + \"\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"), \n",
    "        #             fontsize=14, fontweight='bold', ha='center', va='top')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(folder + '/' + test_runNum + '-' + mode + '_randomNormCheck' + str(i) + '.png')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extension for report figure\n",
    "def normalized_illustration_randomcheck_Figure4_1(run_data,samplingsize,mode):\n",
    "    data = random.choice(run_data)\n",
    "    row = data.sample(samplingsize)\n",
    "\n",
    "    oriImage_all = []\n",
    "    mean_all = []\n",
    "    meanStd_all = []\n",
    "    thirdNorm_all = []\n",
    "    \n",
    "    nt = time.strftime(\"%d-%m-%Y_%H%M\")\n",
    "        \n",
    "    folder = dir22 + '/' + test_runNum + '-' + mode + '_randomNormCheck_' + nt\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    for i in row.itertuples():\n",
    "        mean = i.runMeanValue\n",
    "        std = i.runStdValue\n",
    "        \n",
    "        img = mpimg.imread(i.ScreenShotPath)\n",
    "        oriImage_all.append(img)\n",
    "        \n",
    "        imageM = (img - i.runMeanValue)\n",
    "        imageM = imageM.astype(np.float32)\n",
    "        mean_all.append(imageM)\n",
    "\n",
    "        imageMS = (img - i.runMeanValue) / (i.runStdValue)\n",
    "        imageMS = imageMS.astype(np.float32)\n",
    "        meanStd_all.append(imageMS)\n",
    "        \n",
    "        image = (img - i.runMeanValue) / (i.runStdValue)\n",
    "        image = image.astype(np.float32)\n",
    "        image /= 127.5 # To downscale it from (0-255) to \n",
    "        image -= 1.    # (-1 to 1)\n",
    "        image = (image - np.min(image))/(np.max(image) - np.min(image)) # To convert the range into (0 to 1)\n",
    "        thirdNorm_all.append(image)\n",
    "        \n",
    "\n",
    "    oriImage_all= np.asarray(oriImage_all)\n",
    "    mean_all= np.asarray(mean_all)\n",
    "    meanStd_all= np.asarray(meanStd_all)\n",
    "    thirdNorm_all= np.asarray(thirdNorm_all)\n",
    "    \n",
    "    imageLen = list(range(0,(oriImage_all.shape[0]),1))\n",
    "    \n",
    "    for i in imageLen:\n",
    "        fig, axes = plt.subplots(2,2, figsize=(15,8))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for idx, ax in enumerate(axes):\n",
    "            if idx == 0:\n",
    "                dataname = 'Original Image'\n",
    "                img = oriImage_all[i]\n",
    "                ax.set_title(dataname, pad=20, fontsize=14, fontweight='bold')                          \n",
    "                ax.matshow(img)\n",
    "            elif idx == 1:\n",
    "                dataname = 'Original Image - Mean'\n",
    "                img = mean_all[i]\n",
    "                ax.set_title(dataname, pad=20, fontsize=14, fontweight='bold')                \n",
    "                ax.matshow(img)\n",
    "            elif idx == 2:#4:\n",
    "                dataname = '(Original Image - Mean)/Std'\n",
    "                img = meanStd_all[i]\n",
    "                ax.set_title(dataname, pad=20, fontsize=14, fontweight='bold')                \n",
    "                ax.matshow(img)\n",
    "            elif idx == 3:\n",
    "                dataname = 'Normalized Image'\n",
    "                img = thirdNorm_all[i]\n",
    "                ax.set_title(dataname, pad=20, fontsize=14, fontweight='bold')                \n",
    "                ax.matshow(img)\n",
    "            \n",
    "        fig.tight_layout()\n",
    "        fig.savefig(folder + '/' + test_runNum + '-' + mode + '_randomNormCheck' + str(i) + '.png')\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple images each batch depend on specified batch size\n",
    "def datagenerator(run_data, samplingsize, angles):\n",
    "    while True:\n",
    "        if angles == 'yes':\n",
    "            data = random.choice(run_data)\n",
    "            row = data.sample(samplingsize)\n",
    "\n",
    "            images = []\n",
    "            xy = []\n",
    "            angles = []\n",
    "\n",
    "            for i in row.itertuples():\n",
    "                img = mpimg.imread(i.ScreenShotPath)\n",
    "                image = (img - i.runMeanValue) / (i.runStdValue) #+ 0.0001) # Group Normalisation # May get the error 'can only concatenate list (not \"float\") to list' if i add 0.0001 although the original intention of doing this is to avoid zero error during model training\n",
    "                x = i.downScalingX\n",
    "                y = i.downScalingY\n",
    "                image = image.astype(np.float32)\n",
    "                # ImageNet Technique (Not necessary, because next step makes it into 0 to 1. 0 to 1 is preferred because negative values will become 0 weight in relu of Conv2D)\n",
    "                # But just placed in since previous testings take this algorithm into account, and no more time to rerun everything before report submission\n",
    "                # Advised to drop it if anyone from the lab is using my code unless your model activation does not change negative values into 0, then maybe you can try using this and cut out next standardization step\n",
    "                image /= 127.5 # To downscale it from (0-255) to \n",
    "                image -= 1.    # (-1 to 1)\n",
    "                image = (image - np.min(image))/(np.max(image) - np.min(image)) # To convert the range into (0 to 1)\n",
    "                images.append(image)\n",
    "                labels = np.array((x,y))\n",
    "                xy.append(labels)\n",
    "\n",
    "                angle = i.Angle \n",
    "                if len(angle) >= 3 and len(angle) <= 4:\n",
    "                    angle = angle[:-1]\n",
    "                elif len(angle) == 7:\n",
    "                    angle = angle[0]   \n",
    "                angle = radians(int(angle)) # To convert degree into radians ##angle = angle / pi # To further convert into pi radians           \n",
    "                angles.append(angle)\n",
    "\n",
    "            images= np.asarray(images)\n",
    "            xy =  np.asarray(xy)\n",
    "            angles =  np.asarray(angles)\n",
    "\n",
    "            yield images, [xy, angles]\n",
    "        \n",
    "        elif angles == 'no':\n",
    "            data = random.choice(run_data)\n",
    "            row = data.sample(samplingsize)\n",
    "\n",
    "            images = []\n",
    "            xy = []\n",
    "\n",
    "            for i in row.itertuples():\n",
    "                img = mpimg.imread(i.ScreenShotPath)\n",
    "                image = (img - i.runMeanValue) / (i.runStdValue) #+ 0.0001) # Group Normalisation # May get the error 'can only concatenate list (not \"float\") to list' if i add 0.0001 although the original intention of doing this is to avoid zero error during model training\n",
    "                x = i.downScalingX\n",
    "                y = i.downScalingY\n",
    "                image = image.astype(np.float32)\n",
    "                # ImageNet Technique (Not necessary, because next step makes it into 0 to 1. 0 to 1 is preferred because negative values will become 0 weight in relu of Conv2D)\n",
    "                # But just placed in since previous testings take this algorithm into account, and no more time to rerun everything before report submission\n",
    "                # Advised to drop it if anyone from the lab is using my code unless your model activation does not change negative values\n",
    "                image /= 127.5 # To downscale it from (0-255) to \n",
    "                image -= 1.    # (-1 to 1)\n",
    "                image = (image - np.min(image))/(np.max(image) - np.min(image)) # To convert the range into (0 to 1)\n",
    "                images.append(image)\n",
    "                labels = np.array((x,y))\n",
    "                xy.append(labels)\n",
    "\n",
    "            images= np.asarray(images)\n",
    "            xy =  np.asarray(xy)\n",
    "\n",
    "            yield images, xy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To preprocess all and distribute for train and test as percentage\n",
    "# Section 3.2.3 of report\n",
    "\n",
    "## To generate the list before feeding into datagenerator ##\n",
    "\n",
    "\n",
    "def dataProcessAll_PercentageTest_Step1(runlist,grid_division,train_percentage):\n",
    "    \n",
    "    run_data = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    for num in runlist: ## Do for every run ##\n",
    "        all_images = []\n",
    "        XY = [] ## The existing XY positions ##\n",
    "        XYnew = [] ## The newly divided XY positions ##\n",
    "        chosenCoordinates = [] ## To choose an array of old position that matches most closely with the new positions ##\n",
    "        \n",
    "        filename_screenshot = (f\"Table_Run{num}.csv\")\n",
    "        run_table = pd.read_csv(dir6 + filename_screenshot, usecols = ['Angle','X','Y','ScreenShotPath']) ## To read at the right csv table ##\n",
    "        \n",
    "        maxX = max(run_table['X']) ## finding the maximum X in old positions ##\n",
    "        minX = min(run_table['X'])\n",
    "        maxY = max(run_table['Y']) ## finding the maximum Y in old positions ##\n",
    "        minY = min(run_table['Y'])\n",
    "\n",
    "        for rt in run_table.itertuples(): ## To make X,Y pairing tuple list ##\n",
    "            all_images.append(mpimg.imread(rt.ScreenShotPath))\n",
    "            label_x = rt.X\n",
    "            label_y = rt.Y\n",
    "            XY.append((label_x, label_y)) ## To make each row of the X and Y into a tuple pair\n",
    "        XY = list(set(XY)) ## To remove duplication in list by converting it to set to speed up comparison later by shrinking the size 8 times ##\n",
    "        all_images = np.array(all_images)\n",
    "        \n",
    "        # Full RGB mean\n",
    "        full_rgb_mean = np.mean(all_images, axis=(0,1)) \n",
    "        full_rgb_std = np.std(all_images, axis=(0,1))\n",
    "        \n",
    "        X = np.linspace(0,maxX,grid_division) ## To get the new X portion of division ##\n",
    "        Y = np.linspace(0,maxY,grid_division) ## To get the new X portion of division ##\n",
    "        mesh = np.meshgrid(X,Y) # To make even distribution across the map\n",
    "        X = mesh[0].flatten() # X-axis\n",
    "        Y = mesh[1].flatten() # Y-axis\n",
    "        XYnew = [[x, y] for x, y in zip(X, Y)] ## To convert each X and Y (in ascending order) into tuple pairs in one list: [(X1,Y1),(X2,Y2)...]\n",
    "            \n",
    "        for grid in XYnew: ## For each tuple XY pair in XYnew list\n",
    "            def distance(point_a, point_b): ## To define Xold - Xnew and Yold-Ynew (Teaching the program to do maths the way you define) ##\n",
    "                x0, y0 = point_a\n",
    "                x1, y1 = point_b\n",
    "                return math.fabs(x0 - x1) + math.fabs(y0 - y1)\n",
    "        \n",
    "            def nearest(grid, XY): ## To get the smallest value from Xold - Xnew and Yold-Ynew ##\n",
    "                distance_from_point = functools.partial(distance, grid)\n",
    "                return min(XY, key=distance_from_point)\n",
    "            chosenCoordinates.append(nearest(grid,XY)) ## To store the coordinates closest to each new position in an array of tuple pairings\n",
    "        \n",
    "        data = []\n",
    "    \n",
    "        for row in run_table.itertuples():\n",
    "            for x,y in chosenCoordinates:\n",
    "                if row.X == x and row.Y == y:\n",
    "                    newArray = row.Index\n",
    "                    data.append(newArray) ## To form a list of indices of training data\n",
    "        data = run_table.iloc[data] # MF: Index into dataframe with chosen indices\n",
    "        \n",
    "        ## To rescale the X and Y\n",
    "        downScalingX_column = []\n",
    "        downScalingY_column = []\n",
    "        runMean_column = []\n",
    "        runStd_column = []\n",
    "        #image_column = []\n",
    "        #downScalingAngle_column = []\n",
    "        #downScalingXYlabel_column = []\n",
    "\n",
    "        for i in data.itertuples():\n",
    "            downScalingX = (i.X - minX)/(maxX - minX)\n",
    "            downScalingX_column.append(downScalingX)\n",
    "            downScalingY = (i.Y - minY)/(maxY - minY)\n",
    "            downScalingY_column.append(downScalingY)\n",
    "\n",
    "            runMeanValue = full_rgb_mean\n",
    "            runMean_column.append(runMeanValue)\n",
    "            runStdValue = full_rgb_std\n",
    "            runStd_column.append(runStdValue)\n",
    "\n",
    "            #label_xy = np.array((downScalingX,downScalingY))\n",
    "            #downScalingXYlabel_column.append(label_xy)\n",
    "\n",
    "            #img = mpimg.imread(i.ScreenShotPath)\n",
    "            #image = (img - runMeanValue) / (runStdValue) #+ 0.0001) # Group Normalisation # May get the error 'can only concatenate list (not \"float\") to list' if i add 0.0001 although the original intention of doing this is to avoid zero error during model training\n",
    "            #image = image.astype(np.float32)\n",
    "            #image /= 127.5 # To downscale it from (0-255) to \n",
    "            #image -= 1.    # (-1 to 1)\n",
    "            #image = (image - np.min(image))/(np.max(image) - np.min(image)) # To convert the range into (0 to 1)\n",
    "            #image = np.asarray(image)\n",
    "            #image_column.append(image)\n",
    "\n",
    "            #angle = i.Angle \n",
    "            #if len(angle) >= 3 and len(angle) <= 4:\n",
    "            #    angle = angle[:-1]\n",
    "            #elif len(angle) == 7:\n",
    "            #    angle = angle[0]           \n",
    "            #downScalingAngle = radians(int(angle)) # To convert degree into radians ##angle = angle / pi # To further convert into pi radians   \n",
    "            #downScalingAngle_column.append(downScalingY)\n",
    "\n",
    "        data = data.assign(downScalingX = downScalingX_column)\n",
    "        data = data.assign(downScalingY = downScalingY_column)\n",
    "        data = data.assign(runMeanValue = runMean_column)\n",
    "        data = data.assign(runStdValue = runStd_column)\n",
    "        #data = data.assign(downScalingXYlabel = downScalingXYlabel_column)\n",
    "        #data = data.assign(normalisedImageNP = image_column)\n",
    "        #data = data.assign(downScalingAngle = downScalingAngle_column)\n",
    "        data = data.sample(frac=1).reset_index(drop=True) # To shuffle sequence between a run\n",
    "        \n",
    "        run_data.append(data)\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "    all_rd = []\n",
    "    rd_len = list(range(0,len(run_data),1))\n",
    "\n",
    "    for i in rd_len:\n",
    "        item = eval(f\"run_data[{i}]\")\n",
    "        all_rd.append(item)\n",
    "    #print(type(all_rd),all_rd)\n",
    "\n",
    "    new_run_data = pd.concat(all_rd)\n",
    "    #new_run_data = new_run_data.sample(frac=1).reset_index(drop=True) # To shuffle sequence across runs\n",
    "    \n",
    "    selected_datafeed_train = round(int(len(new_run_data) * train_percentage))\n",
    "    training_data = new_run_data[:int(selected_datafeed_train)]\n",
    "    testing_data = new_run_data[int(selected_datafeed_train):]\n",
    "    \n",
    "\n",
    "    return training_data,testing_data\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "\n",
    "## Multiple images each batch depend on specified batch size\n",
    "def datagenerator_PercentageTest_Step2(run_data, samplingsize, angles):\n",
    "    while True:\n",
    "        if angles == 'yes':\n",
    "            row = run_data.sample(samplingsize)\n",
    "\n",
    "            images = []\n",
    "            xy = []\n",
    "            angles = []\n",
    "\n",
    "            for i in row.itertuples():\n",
    "                img = mpimg.imread(i.ScreenShotPath)\n",
    "                image = (img - i.runMeanValue) / (i.runStdValue) #+ 0.0001) # Group Normalisation # May get the error 'can only concatenate list (not \"float\") to list' if i add 0.0001 although the original intention of doing this is to avoid zero error during model training\n",
    "                x = i.downScalingX\n",
    "                y = i.downScalingY\n",
    "                image = image.astype(np.float32)\n",
    "                image /= 127.5 # To downscale it from (0-255) to \n",
    "                image -= 1.    # (-1 to 1)\n",
    "                image = (image - np.min(image))/(np.max(image) - np.min(image)) # To convert the range into (0 to 1)\n",
    "                images.append(image)\n",
    "                labels = np.array((x,y))\n",
    "                xy.append(labels)\n",
    "\n",
    "                angle = i.Angle \n",
    "                if len(angle) >= 3 and len(angle) <= 4:\n",
    "                    angle = angle[:-1]\n",
    "                elif len(angle) == 7:\n",
    "                    angle = angle[0]   \n",
    "                angle = radians(int(angle)) # To convert degree into radians ##angle = angle / pi # To further convert into pi radians           \n",
    "                angles.append(angle)\n",
    "\n",
    "            images= np.asarray(images)\n",
    "            xy =  np.asarray(xy)\n",
    "            angles =  np.asarray(angles)\n",
    "\n",
    "            yield images, [xy, angles]\n",
    "        \n",
    "        elif angles == 'no':\n",
    "            row = run_data.sample(samplingsize)\n",
    "\n",
    "            images = []\n",
    "            xy = []\n",
    "\n",
    "            for i in row.itertuples():\n",
    "                img = mpimg.imread(i.ScreenShotPath)\n",
    "                image = (img - i.runMeanValue) / (i.runStdValue) #+ 0.0001) # Group Normalisation # May get the error 'can only concatenate list (not \"float\") to list' if i add 0.0001 although the original intention of doing this is to avoid zero error during model training\n",
    "                x = i.downScalingX\n",
    "                y = i.downScalingY\n",
    "                image = image.astype(np.float32)\n",
    "                image /= 127.5 # To downscale it from (0-255) to \n",
    "                image -= 1.    # (-1 to 1)\n",
    "                image = (image - np.min(image))/(np.max(image) - np.min(image)) # To convert the range into (0 to 1)\n",
    "                images.append(image)\n",
    "                labels = np.array((x,y))\n",
    "                xy.append(labels)\n",
    "\n",
    "            images= np.asarray(images)\n",
    "            xy =  np.asarray(xy)\n",
    "\n",
    "            yield images, xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3.2.2 of report (Shuffling of snapshots)\n",
    "\n",
    "## To generate the list before feeding into datagenerator ##\n",
    "\n",
    "def datagenerator_prep1_extractdata_Shuffle_250520(runMean,runStd,runlist,grid_division,mode):\n",
    "    \n",
    "    if mode == 'training' or mode == 'learning':\n",
    "        chosen_angle = ['45°','135°','225°','315°'] # X\n",
    "        #chosen_angle = ['315°','0°/360°','45°','90°'] # Horizontal Upper\n",
    "        #chosen_angle = ['315°','0°/360°','45°'] # Horizontal Upper, just 3 angles\n",
    "        #chosen_angle = ['0°/360°','45°','90°','135°'] # Vertical right\n",
    "    elif mode == 'testing':\n",
    "        chosen_angle = ['0°/360°','90°','180°','270°'] # +\n",
    "        #chosen_angle = ['135°','180°','225°','270°'] # Horizontal lower\n",
    "        #chosen_angle = ['135°','180°','225°'] # Horizontal lower, just 3 angles\n",
    "        #chosen_angle = ['180°','225°','270°','315°'] # Vertical left\n",
    "    else: ## In case user forget to define the mode, the program will be halted\n",
    "        raise ValueError('Please select mode of paradigm')\n",
    "        pass\n",
    "    \n",
    "    run_data = []\n",
    "    \n",
    "    index = 0\n",
    "    \n",
    "    for num in runlist: ## Do for every run ##\n",
    "        XY = [] ## The existing XY positions ##\n",
    "        XYnew = [] ## The newly divided XY positions ##\n",
    "        chosenCoordinates = [] ## To choose an array of old position that matches most closely with the new positions ##\n",
    "        \n",
    "        filename_screenshot = (f\"Table_Run{num}.csv\")\n",
    "        run_table = pd.read_csv(dir6 + filename_screenshot, usecols = ['Angle','X','Y','ScreenShotPath']) ## To read at the right csv table ##\n",
    "        \n",
    "        ##################################\n",
    "        ## Shuffling here ##\n",
    "        \n",
    "        # run_table = run_table.sample(frac=1).reset_index(drop=True) # <<<< This only helps to reorder the whole row, and not row and every column to make mismatch\n",
    "        # equal to run_table.reindex(np.random.permutation(run_table.index))\n",
    "        # Difference between loc, iloc and ix: https://stackoverflow.com/questions/31593201/how-are-iloc-ix-and-loc-different\n",
    "        \n",
    "        angle = run_table['Angle']#.sample(frac=1).reset_index(drop=True)\n",
    "        x = run_table['X']#.sample(frac=1).reset_index(drop=True)\n",
    "        y = run_table['Y']#.sample(frac=1).reset_index(drop=True)\n",
    "        ssPath = run_table['ScreenShotPath'].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        #shuffled = [angle, x, y, ssPath]\n",
    "        #rt = pd.concat(shuffled)\n",
    "\n",
    "        data = {'Angle':angle,'X':x,'Y':y,'ScreenShotPath':ssPath}\n",
    "        df = pd.DataFrame(data, columns=['Angle','X','Y','ScreenShotPath'])\n",
    "        \n",
    "        run_table = df\n",
    "        \n",
    "        #import gc\n",
    "        del df # deleting the variable df to save some RAM\n",
    "        \n",
    "        ##################################\n",
    "        \n",
    "        maxX = max(run_table['X']) ## finding the maximum X in old positions ##\n",
    "        minX = min(run_table['X'])\n",
    "        maxY = max(run_table['Y']) ## finding the maximum Y in old positions ##\n",
    "        minY = min(run_table['Y'])\n",
    "\n",
    "        for rt in run_table.itertuples(): ## To make X,Y pairing tuple list ##\n",
    "            label_x = rt.X\n",
    "            label_y = rt.Y\n",
    "            XY.append((label_x, label_y)) ## To make each row of the X and Y into a tuple pair\n",
    "        XY = list(set(XY)) ## To remove duplication in list by converting it to set to speed up comparison later by shrinking the size 8 times ##\n",
    "\n",
    "        X = np.linspace(0,maxX,grid_division) ## To get the new X portion of division ##\n",
    "        Y = np.linspace(0,maxY,grid_division) ## To get the new X portion of division ##\n",
    "        mesh = np.meshgrid(X,Y) # To make even distribution across the map\n",
    "        X = mesh[0].flatten() # X-axis\n",
    "        Y = mesh[1].flatten() # Y-axis\n",
    "        XYnew = [[x, y] for x, y in zip(X, Y)] ## To convert each X and Y (in ascending order) into tuple pairs in one list: [(X1,Y1),(X2,Y2)...]\n",
    "            \n",
    "        for grid in XYnew: ## For each tuple XY pair in XYnew list\n",
    "            def distance(point_a, point_b): ## To define Xold - Xnew and Yold-Ynew (Teaching the program to do maths the way you define) ##\n",
    "                x0, y0 = point_a\n",
    "                x1, y1 = point_b\n",
    "                return math.fabs(x0 - x1) + math.fabs(y0 - y1)\n",
    "        \n",
    "            def nearest(grid, XY): ## To get the smallest value from Xold - Xnew and Yold-Ynew ##\n",
    "                distance_from_point = functools.partial(distance, grid)\n",
    "                return min(XY, key=distance_from_point)\n",
    "            chosenCoordinates.append(nearest(grid,XY)) ## To store the coordinates closest to each new position in an array of tuple pairings\n",
    "        \n",
    "        data = []\n",
    "    \n",
    "        for row in run_table.itertuples():\n",
    "            for x,y in chosenCoordinates:\n",
    "                if row.X == x and row.Y == y and any(row.Angle in angle for angle in chosen_angle):\n",
    "                    newArray = row.Index\n",
    "                    data.append(newArray) ## To form a list of indices of training data\n",
    "        data = run_table.iloc[data] # MF: Index into dataframe with chosen indices\n",
    "        \n",
    "        ## To rescale the X and Y\n",
    "        downScalingX_column = []\n",
    "        downScalingY_column = []\n",
    "        runMean_column = []\n",
    "        runStd_column = []\n",
    "        \n",
    "        for i in data.itertuples():\n",
    "            downScalingX = (i.X - minX)/(maxX - minX)\n",
    "            downScalingX_column.append(downScalingX)\n",
    "            downScalingY = (i.Y - minY)/(maxY - minY)\n",
    "            downScalingY_column.append(downScalingY)\n",
    "            runMeanValue = runMean[index]\n",
    "            runMean_column.append(runMeanValue)\n",
    "            runStdValue = runStd[index]\n",
    "            runStd_column.append(runStdValue)\n",
    "            \n",
    "        \n",
    "        data = data.assign(downScalingX = downScalingX_column)\n",
    "        data = data.assign(downScalingY = downScalingY_column)\n",
    "        data = data.assign(runMeanValue = runMean_column)\n",
    "        data = data.assign(runStdValue = runStd_column)\n",
    "        \n",
    "        run_data.append(data)\n",
    "        \n",
    "        index += 1\n",
    "\n",
    "    return run_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
